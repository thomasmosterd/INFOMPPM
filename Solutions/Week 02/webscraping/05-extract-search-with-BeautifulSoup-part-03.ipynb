{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract search results with BeautifulSoup: PBS.org - part 03\n",
    "In our previous Notebook we scraped only one page of the results. At the time of writing there we 30 pages of results. By adding an extra for-loop to the code we will be able to traverse through all the pages. But before we do this we will make the code dynamic so you can scrape multiple keywords from the site (if you want to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieve how many pages there are\n",
    "This will vary per website but luckily PBS.org displays the final page in the pagination overview. If you click on a link you see the URL of your browser changes into something like:\n",
    "`https://www.pbs.org/newshour/search-results?q=%22artificial+intelligence%22&pnb=2` where `&pnb=2` is the current page. Again, this will change from site to site but for now it is a welcome way to scrape.\n",
    "So now we need to know how many pages there are. Looking at the HTML code the best strategy is to get the last item of the class `pagination__number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we need the %22 or \" to ensure that we get the combination artificial intelligence\n",
    "url = 'https://www.pbs.org/newshour/search-results?q=%22artificial%20intelligence%22'\n",
    "\n",
    "# get url\n",
    "page = requests.get(url)\n",
    "\n",
    "# transform to soup\n",
    "soup = BeautifulSoup(page.content, 'html')\n",
    "\n",
    "# search for pagination links\n",
    "pages = soup.find_all(class_='pagination__number')\n",
    "\n",
    "# [-1] selects last item in a list\n",
    "last_page = pages[-1].get_text()\n",
    "\n",
    "# convert to int\n",
    "number_of_pages = int(last_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create URL list\n",
    "Now we have our total number of pages we can create a nice url list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "\n",
    "for i in range(number_of_pages):\n",
    "  url = 'https://www.pbs.org/newshour/search-results?q=%22artificial+intelligence%22&pnb=' + str(i+1)  \n",
    "  url_list.append(url)\n",
    "\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieve all the article URLs and save them in a list them\n",
    "Normally you could save this into a seperate file but since we are using Jupyter Notebooks we can temporary store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "article_list = []\n",
    "\n",
    "for url in url_list:\n",
    "  print('Retrieving',url)\n",
    "  # get url\n",
    "  page = requests.get(url)\n",
    "\n",
    "  # transform to soup\n",
    "  soup = BeautifulSoup(page.content, 'html')\n",
    "\n",
    "  results = soup.find_all(class_='search-result__title')\n",
    "\n",
    "  for res in results:\n",
    "    # you can search in the extracted data by referencing the extracted data\n",
    "    url = res.find('a')['href']\n",
    "    article_list.append(url)\n",
    "\n",
    "  time.sleep(2)\n",
    "\n",
    "article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Go through the list of articles and save the individual files.\n",
    "Look at the previous Notebooks in order to solve this part. Don't forget to use `article_list`. This can take some time to complete Â±15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in article_list:\n",
    "  print(\"Retrieving\",url)\n",
    "\n",
    "  # get url\n",
    "  page = requests.get(url)\n",
    "  \n",
    "  # create a sensible filename\n",
    "  filename = url.replace('https://www.pbs.org/newshour/', '').replace('/', '-') + '.html'\n",
    "  destination = './data/' + filename\n",
    "    \n",
    "  with open(destination, 'w') as f:\n",
    "    f.write(page.text)\n",
    "  \n",
    "  # wait two seconds not to overuse the server\n",
    "  time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37c10f95d263926787ebf1d430d11186fc6b9bac835b8518e0b5006ed24f0c36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
