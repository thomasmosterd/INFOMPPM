{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract with BeautifulSoup: PBS.org\n",
    "Now that you can collect the contents of a single HTML file, it is time to extract data from it. For this, we will use the package [BeautifulSoup4](https://pypi.org/project/beautifulsoup4/). BeautifulSoup is a library that makes it easy to scrape information from web pages. It transforms the textual HTML files into an object, iterating, searching, and modifying the parse tree—the easiest way to extract data from HTML files.\n",
    "\n",
    "In this Notebook, you will learn how to:\n",
    "1. collect data\n",
    "2. extract data based on HTML element\n",
    "3. extract data based on the class attribute\n",
    "4. extract data based on another HTML attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# retrieve an individual article\n",
    "url = 'https://www.pbs.org/newshour/economy/google-ceo-calls-for-regulation-of-artificial-intelligence'\n",
    "\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read file with BeautifulSoup\n",
    "The `page.content` from `requests` is used and transformed into a BS4 object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# transform into a BS4 object\n",
    "soup = BeautifulSoup(page.content, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract the title\n",
    "Now that we have an `soup` object, we can fire queries to extract the data. There is extensive [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) about BS4, so look at it when in doubt. This Notebook will cover the basics. This part also requires some familiarisation with HTML and CSS. We will discuss parts in class, but a good primer can be found [here](https://www.w3schools.com/). For now we are going to extract content from the `<title></title>` element.\n",
    "\n",
    "Finding HTML elements such as `<h1></h1>` or `<title></title>` can easily be done with `soup.find()`. The response, in this case will be `<title>Google CEO calls for regulation of artificial intelligence | PBS NewsHour</title>`. Use the `.get_text()` method to extract the text from the `<title/>` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the <title></title> in the document. \n",
    "title = soup.find('title').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning up the response\n",
    "Cleaning is a large part of Web data extraction. In this case we want to get rid of the ` | PBS NewsHour` and will use the versatile String method `.replace()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title.replace(' | PBS NewsHour', '')\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Getting more data\n",
    "Unfortunately, not all data is as easy to extract as the title. Consider elements such as `<a>`, `<span>`, or `<div>` that can occur many times. Let us extract the author of this article. Go to your browser and determine where the content is on the page. Right-click on the link and inspect the element.\n",
    "\n",
    "If everything works as expected you will see that the element is `<a class=\"post__byline-name-hyphenated\" href=\"https://www.pbs.org/newshour/author/kelvin-chan-associated-press\">Kelvin Chan, Associated Press</a>`. How would you extract this? Searching for an `<a>` element will return a value but will it be the right value? In this case, it will search and return the first one it finds. \n",
    "\n",
    "A way to avoid this is by searching for the attributes of an HTML element. In this case the CSS class `post__byline-name-hyphenated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = soup.find(class_='post__byline-name-hyphenated')\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5a. Extract and clean the author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = soup.find(class_='post__byline-name-hyphenated')\n",
    "author = author.get_text().replace(', Associated Press', '').strip()\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5b. Extracting the date and time\n",
    "A quick inspection shows that there is a `<time>` element you can extract the date and time: `<time class=\"post__date\" itemprop=\"datePublished\" content=\"2020-01-20T09:13:07-05:00\">Jan 20, 2020 9:13 AM EST</time>`. You have two opions:\n",
    "1. extract the text data `Jan 20, 2020 9:13 AM EST` and transform it into a datetime format\n",
    "2. extract the info from the `content=\"2020-01-20T09:13:07-05:00\"` attribute.\n",
    "\n",
    "Let us explore the second option. In this case, there is only one `<time>` so we can use `soup.find()`. Extracting the attribute by adding `['content']` after the `.find()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime = soup.find('time')['content']\n",
    "datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5c. Extracting the post category\n",
    "With the knowledge you have know you can extract the post category. Do so below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Meta data\n",
    "If you are in luck, and in this case, you are, the website has metadata describing the post. This makes life a lot easier. Go to the web page and inspect the HTML code by inspecting the page. Go to the `<head>` element on top of the code. Here you will see all different meta elements with properties such as type, title, description, image, section, etc. \n",
    "\n",
    "In this case, we cannot use a CSS class as we did earlier. Luckily, we can search for specific attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('meta',  attrs={'property': 'og:title'})['content']\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6a. Extract the meta data and save in a dictionary\n",
    "Make sure you have the following dictionary:\n",
    "`{'title': 'Google CEO calls for regulation of artificial intelligence', 'description': '“There is no question in my mind that artificial intelligence needs to be regulated. The question is how best to approach this,” Sundar Pichai said.','datetime': '2020-01-20T09:13:07-05:00', 'section': 'Economy', 'tags': 'artificial intelligence, google, technology'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37c10f95d263926787ebf1d430d11186fc6b9bac835b8518e0b5006ed24f0c36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
