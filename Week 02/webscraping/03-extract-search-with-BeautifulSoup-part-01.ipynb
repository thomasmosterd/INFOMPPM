{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract search results with BeautifulSoup: PBS.org - part 01\n",
    "Now that you have a basic understanding of extracting a single page's data, we will continue extracting search results to extract multiple pages. In this Notebook, we will take a look at:\n",
    "1. Multiple elements\n",
    "2. Saving the multiple pages locally for data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Looking at the search results\n",
    "In this case, we are interested in [\"artificial intelligence\"]('https://www.pbs.org/newshour/search-results?q=%22artificial%20intelligence%22') and will search PBS Newshour for articles containing the concept. Collect the first page of the search results and create a BS4 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting multiple elements\n",
    "We are interested in the URLs that link to the individual articles we want to save later. In the previous Notebook, you looked at single items but, in this Notebook, multiple. Luckily, these are pretty simple to find, and a quick scan reveals we need to use `<h4 class=\"search-result__title\"> ... </h4>` since this element contains the link to the article. BS4 is flexible, and you can search within extracted snippets of HTML.\n",
    "\n",
    "First we need to `.find_all()` items with a class. This will return a list of snippets extracted from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all(class_='search-result__title')\n",
    "\n",
    "for res in results:\n",
    "  # print extracted snippet\n",
    "  print(res)\n",
    "  \n",
    "  # you can search in the extracted data by referencing the extracted data\n",
    "  title = res.find('a').get_text()\n",
    "  \n",
    "  print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2a. Extract the urls and save in a list\n",
    "Attribute values can be extracted by simply adding them to the `.find(x)['attr']`. Determine which of the attribute contains the URL, extract the URL and add this to `url_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all(class_='search-result__title')\n",
    "\n",
    "url_list = []\n",
    "\n",
    "for res in results:\n",
    "  # code goes here\n",
    "\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save the articles in the data folder\n",
    "While retrieving and extracting simultaneously is an option, it is less practical. Saving a file to the disk has the following advantages:\n",
    "1. You do not overuse the website\n",
    "2. If there is an error extracting (for instance, the page is slightly different), then you can easily redo without starting from the beginning\n",
    "3. You have an archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use time.sleep() to make the script wait\n",
    "import time\n",
    "\n",
    "# now we iterate through the url list and save the individual pages\n",
    "for url in url_list:\n",
    "  print(\"Retrieving\",url)\n",
    "\n",
    "  # get url\n",
    "  page = requests.get(url)\n",
    "  \n",
    "  # create a sensible filename\n",
    "  filename = url.replace('https://www.pbs.org/newshour/', '').replace('/', '-') + '.html'\n",
    "  destination = './data/' + filename\n",
    "    \n",
    "  with open(destination, 'w') as f:\n",
    "    f.write(page.text)\n",
    "  \n",
    "  # wait two seconds not to overuse the server\n",
    "  time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37c10f95d263926787ebf1d430d11186fc6b9bac835b8518e0b5006ed24f0c36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
